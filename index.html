<!DOCTYPE html>
<html lang="en">
<head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="TODO">
    <meta name="author" content="TODO">

    <meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate">
    <meta http-equiv="Pragma" content="no-cache">
    <meta http-equiv="Expires" content="0">



	<link href="style.css" rel="stylesheet">
	<title>Mapping the Audio Landscape for Innovative Music Sample Generation</title>

	<script src="js/audio_demo.js"></script>

  <link href="https://fonts.google
  apis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">


</head>

<body>
	
<div id="outer">
	<div id="inner">

				
			<div id="headsection"><div id="inner_headsection">
			<h1 id="title">Transformer-based Audio Generation conditioned by 2D Latent Maps - A Demonstration</h1>
				  
				  
			<h3 id="authors">Authors: Christian Limberg and Zhe Zhang</h3>
				
	  
			<h2 id="abstract">Abstract</h2>
			<p id="abstract_txt">
				

				This paper presents a demonstration of an improved framework for audio sample generation using interactive 2D latent maps. Building upon the foundational work "Mapping the Audio Landscape for Innovative Music Sample Generation," we enhance the framework by introducing visualization techniques for exploring the 2D audio landscape through different audio features such as energy and bandwidth. Additionally, we train a t-SNE embedding over these features to create a more abstract visualization of the audio samples on the map. This demo also significantly improves usability and user interactivity, allowing for a more intuitive and efficient exploration of the generated audio samples. The demo on this website showcases these improvements in real-time, providing users with an enhanced interface for generating high-quality audio samples.



				
				
<!--
% 1. Satz: Narrativ, was wird gemacht? Reason for writing/reading
% 2. Satz: Was ist das Problem und was ist gerade Stand der Technick? +
% 3. Satz: Was macht man anders als der Stand der Technik, was ist die eigene Methologie
% 4. Satz: Was sind die Ergebnisse?
% 5. Was implizieren die Ergebnisse fuer die Zukunft? How does this work add to the body of knowledge on the topic?


This article introduces a Transformer-based model capable of generating high quality audio samples for music production by conditioning the model with a coordinate on a 2d sample map. 

Especially the audio domain is difficult for generative models because of long sequence lengths. Especially for music production it is hard because samples have to have a high quality to be used within songs.

State of the art approaches primarily condition sound by a text embedding which is not very suitable for musical sample generation since samples differ by nuances that can not be expressed by text. 
It is more natural for the human to describe an abstract thing with words as 'closer to this' or 'I want something more in this direction'. 

To achieve this we conditioned a transformer with a trained 2d embedding for querying audio samples more intuitively and efficient. 

By our results, which we demonstrate by an interactive web-demo (LINK), we hope to broaden the perspective on how to query audio samples and how a future digital audio workstation could be. 
-->

			</p>
			

			
			</div></div>



			<div class="figure"><div class="inner_figure">
				<a id="fig1"></a> 
				<h3 class="figure_title"> Audio Sample Generator </h3>






				<div class="selection" onclick="activateDiv('classifier_selection')">Display Classifier Colors</div>
				<div id="classifier_selection" class="content-div">

					<div class="explanation">
						We trained a classifier that can distinguishes the classes we trained the transformer model with. We color the embedding space with the corresponding class color by using a 5 nearest neighbor classifier, where we mix the colors of the 5 nearest neighbors for creating a visual transition between classes. 
					</div>

					<h4>Sample Classes: </h4>
					<div id="classes_div">
					
					</div>

					<input type="checkbox" id="toggledata" checked/> Show/Hide Training Data


				</div>
			
				<div class="selection" onclick="activateDiv('feature_selection')">Display Audio Features</div>
				<div id="feature_selection" class="content-div">
					
					<div class="explanation">
						We computed 5 different audio features and use a color map for visualizing each one as a background image for orientation. 
					</div>

					<div id="bg_selection">

					</div>

					<input type="checkbox" id="overlaymaps"/>Overlay audio features and classifier map


				</div>
			
				<div class="selection" onclick="activateDiv('tsne_selection')">3D T-SNE</div>
				<div id="tsne_selection" class="content-div">

					<div class="explanation">
						Based on the 5 audio features we trained a 3D T-SNE embedding which we map to RGB space for visualizing a richer, but more abstract map of the sound scape. 
					</div>


				</div>
			

				<div id="selection_div">
					<canvas id="selection_canvas"> </canvas>
					<svg viewBox="-1 -1 2 2" id="selection_svg"> </svg>	
				</div>



				<div class="figure_caption">
					Our interactive interface, lets you generate musical samples by selecting points on a 2d plane. A fraction of the training data is displayed for orientation. The background is colored in the average colors of the nearest neighboring samples. Click on a certain location for playing the related sample (speaker or headphones required).
				</div>

			</div></div>

			<div class="figure"><div class="inner_figure">
				<a id="fig2"></a> 
				<h3 class="figure_title"> Model Architecture </h3>

				<img src="gfx/gesam.png" alt="Model Architecture" style="width:100%;">

				<div class="figure_caption">
					This schematic depicts the training procedure of our model. In the first stage, a VAE with a 2d latent bottleneck is trained. In the second stage the Transformer model is trained with the VAE as a conditioning model.
				</div>

			</div></div>




			<div class="figure"><div class="inner_figure">
			<a id="citeus"></a> 

			<h3 class="figure_title">Cite Us</h3>
			<pre id="citebox">
				This demo paper has to be accepted first before a reference will spawn here.
				</pre>

		</div></div>









		</div> <!-- inner -->


	</div> <!-- outer -->


</body>


</html>
